"""
Haystack 2.x ã‚’ç”¨ã„ãŸ RAG ã‚·ã‚¹ãƒ†ãƒ 

- ChromaDB æ°¸ç¶šåŒ– + OpenAI Embedding ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢
- Pipeline ã‚¯ãƒ©ã‚¹ã«ã‚ˆã‚‹ã‚¤ãƒ³ã‚¸ã‚§ã‚¹ã‚·ãƒ§ãƒ³ / ã‚¯ã‚¨ãƒªãƒ•ãƒ­ãƒ¼ã®æ˜ç¤ºçš„è¨˜è¿°
- å„ã‚¹ãƒ†ãƒƒãƒ—ã®ãƒ‡ãƒ¼ã‚¿å…¥å‡ºåŠ›ãƒ­ã‚°ã«ã‚ˆã‚‹ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å¯è¦–åŒ–

LlamaIndex ç‰ˆ (rag_mark-1/main.py) ã¨åŒç­‰ã®æ©Ÿèƒ½ã‚’ Haystack 2.x ã§å†å®Ÿè£…ã€‚
"""

import argparse
import logging
import os
import re
import shutil
import sys
from pathlib import Path
from typing import Dict, List, Optional

import fitz  # PyMuPDF
from dotenv import load_dotenv

from haystack import Document, Pipeline, component
from haystack.components.builders import PromptBuilder
from haystack.components.embedders import OpenAIDocumentEmbedder, OpenAITextEmbedder
from haystack.components.generators import OpenAIGenerator
from haystack.components.preprocessors import DocumentCleaner
from haystack.components.writers import DocumentWriter
from haystack_integrations.components.retrievers.chroma import ChromaEmbeddingRetriever
from haystack_integrations.document_stores.chroma import ChromaDocumentStore


# â”€â”€â”€ ç’°å¢ƒå¤‰æ•° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
load_dotenv()
# ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: rag_mark-1 ã® .env ã‚‚å‚ç…§
if not os.getenv("OPENAI_API_KEY"):
    _fallback_env = Path(__file__).resolve().parent.parent / "rag_mark-1" / ".env"
    if _fallback_env.exists():
        load_dotenv(_fallback_env)

# â”€â”€â”€ ãƒ­ã‚°è¨­å®š â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
logging.basicConfig(
    level=logging.WARNING,  # Haystack å†…éƒ¨ãƒ­ã‚°ã¯ WARNING ä»¥ä¸Šã®ã¿
    format="%(asctime)s [%(name)s] %(message)s",
    datefmt="%H:%M:%S",
    stream=sys.stdout,
)
# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å¯è¦–åŒ–ç”¨ãƒ­ã‚¬ãƒ¼ï¼ˆINFO ãƒ¬ãƒ™ãƒ«ã§å‡ºåŠ›ï¼‰
pipeline_logger = logging.getLogger("rag_pipeline")
pipeline_logger.setLevel(logging.INFO)

# â”€â”€â”€ å®šæ•° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DATA_DIR = Path("./data")
# ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: rag_mark-1 ã® data ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
if not DATA_DIR.exists():
    _fallback_data = Path(__file__).resolve().parent.parent / "rag_mark-1" / "data"
    if _fallback_data.exists():
        DATA_DIR = _fallback_data

# ChromaDB æ°¸ç¶šåŒ–è¨­å®š
STORAGE_DIR = Path("./storage")
COLLECTION_NAME = "rag_collection"

# ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²è¨­å®šï¼ˆæ–‡å­—æ•°ãƒ™ãƒ¼ã‚¹ã€LlamaIndex ç‰ˆã® 1000 ãƒˆãƒ¼ã‚¯ãƒ³ã«ç›¸å½“ï¼‰
CHUNK_SIZE = 1000
CHUNK_OVERLAP = 300

# ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢è¨­å®š
EMBEDDING_MODEL = "text-embedding-3-large"
TOP_K = 10

# â”€â”€â”€ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SYSTEM_PROMPT = (
    "ã‚ãªãŸã¯æä¾›ã•ã‚ŒãŸè³‡æ–™ã«åŸºã¥ã„ã¦å›ç­”ã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚\n"
    "è³‡æ–™ã«ç­”ãˆãŒãªã„å ´åˆã¯ã€æ¨æ¸¬ã›ãšã€Œè³‡æ–™ã«ã¯è¨˜è¼‰ãŒã‚ã‚Šã¾ã›ã‚“ã€ã¨æ˜ç¢ºã«ä¼ãˆã¦ãã ã•ã„ã€‚\n"
    "å›ç­”ã«ã¯ã€å‚ç…§ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«åã‚’å¿…ãšå«ã‚ã¦ãã ã•ã„ã€‚"
)

RAG_PROMPT_TEMPLATE = """\
ä»¥ä¸‹ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã‚’å‚è€ƒã«ã—ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚

## ã‚·ã‚¹ãƒ†ãƒ æŒ‡ç¤º
{{ system_prompt }}

## ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
{% for doc in documents %}
--- ã‚½ãƒ¼ã‚¹: {{ doc.meta.get("file_name", "ä¸æ˜") }} (ãƒšãƒ¼ã‚¸ {{ doc.meta.get("page_number", "N/A") }}) ---
{{ doc.content }}
{% endfor %}

## ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•
{{ query }}

## å›ç­”
"""


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  ã‚«ã‚¹ã‚¿ãƒ ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ: ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å¯è¦–åŒ–ç”¨ãƒ­ã‚¬ãƒ¼
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@component
class DocumentLogger:
    """
    Document ãƒªã‚¹ãƒˆã‚’ãƒ­ã‚°å‡ºåŠ›ã—ã€ãã®ã¾ã¾æ¬¡ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã¸æ¸¡ã™ãƒ‘ã‚¹ã‚¹ãƒ«ãƒ¼ã€‚
    ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å†…ã«ã“ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’æŒŸã‚€ã“ã¨ã§ã€å„ã‚¹ãƒ†ãƒƒãƒ—ã®å…¥å‡ºåŠ›ã‚’å¯è¦–åŒ–ã™ã‚‹ã€‚
    """

    def __init__(self, step_name: str, max_preview: int = 80):
        self.step_name = step_name
        self.max_preview = max_preview

    @component.output_types(documents=List[Document])
    def run(self, documents: List[Document]) -> dict:
        log = pipeline_logger
        border = "=" * 60
        log.info(border)
        log.info(f"ğŸ“‹ [{self.step_name}] ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {len(documents)}")

        for i, doc in enumerate(documents[:5]):
            preview = doc.content[:self.max_preview].replace("\n", "â†µ")
            meta_keys = {
                k: v for k, v in doc.meta.items()
                if k in ("file_name", "page_number", "chunk_index", "source_doc_id")
            }
            score_str = f", score={doc.score:.4f}" if doc.score is not None else ""
            log.info(f"  [{i + 1}] meta={meta_keys}{score_str}")
            log.info(f"       \"{preview}â€¦\"")
            log.info(f"       (æ–‡å­—æ•°: {len(doc.content)})")
        if len(documents) > 5:
            log.info(f"  â€¦ ä»– {len(documents) - 5} ä»¶çœç•¥")
        log.info(border)

        return {"documents": documents}


@component
class QueryLogger:
    """ã‚¯ã‚¨ãƒªæ–‡å­—åˆ—ã‚’ãƒ­ã‚°å‡ºåŠ›ã—ã€ãã®ã¾ã¾æ¬¡ã¸æ¸¡ã™ãƒ‘ã‚¹ã‚¹ãƒ«ãƒ¼ã€‚"""

    def __init__(self, step_name: str):
        self.step_name = step_name

    @component.output_types(query=str)
    def run(self, query: str) -> dict:
        pipeline_logger.info("=" * 60)
        pipeline_logger.info(f"ğŸ” [{self.step_name}] ã‚¯ã‚¨ãƒª: \"{query}\"")
        pipeline_logger.info("=" * 60)
        return {"query": query}


@component
class GenerationLogger:
    """LLM ç”Ÿæˆçµæœã‚’ãƒ­ã‚°å‡ºåŠ›ã—ã€ãã®ã¾ã¾æ¬¡ã¸æ¸¡ã™ãƒ‘ã‚¹ã‚¹ãƒ«ãƒ¼ã€‚"""

    def __init__(self, step_name: str, max_preview: int = 200):
        self.step_name = step_name
        self.max_preview = max_preview

    @component.output_types(replies=List[str])
    def run(self, replies: List[str]) -> dict:
        log = pipeline_logger
        log.info("=" * 60)
        log.info(f"ğŸ¤– [{self.step_name}] ç”Ÿæˆãƒ¬ã‚¹ãƒãƒ³ã‚¹æ•°: {len(replies)}")
        if replies:
            preview = replies[0][:self.max_preview].replace("\n", "â†µ")
            log.info(f"  ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼: \"{preview}â€¦\"")
        log.info("=" * 60)
        return {"replies": replies}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  ã‚«ã‚¹ã‚¿ãƒ ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ: æ—¥æœ¬èªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåˆ†å‰²
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@component
class JapaneseDocumentSplitter:
    """
    æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã‚’æ–‡å­—æ•°ãƒ™ãƒ¼ã‚¹ã§åˆ†å‰²ã™ã‚‹ã‚«ã‚¹ã‚¿ãƒ ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã€‚
    å¥ç‚¹ã€Œã€‚ã€ã‚„æ”¹è¡Œã§æ–‡ã‚’åŒºåˆ‡ã‚Šã¤ã¤ã€æŒ‡å®šã‚µã‚¤ã‚ºã§ãƒãƒ£ãƒ³ã‚¯åŒ–ã™ã‚‹ã€‚

    Haystack æ¨™æº–ã® DocumentSplitter ã¯è‹±èªã®ç©ºç™½åŒºåˆ‡ã‚Šã‚’å‰æã¨ã—ã¦ã„ã‚‹ãŸã‚ã€
    æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã«ã¯æœ¬ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ä½¿ç”¨ã™ã‚‹ã€‚
    """

    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 300):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap

    @component.output_types(documents=List[Document])
    def run(self, documents: List[Document]) -> dict:
        result = []
        for doc in documents:
            chunks = self._split_text(doc.content)
            for idx, chunk in enumerate(chunks):
                new_meta = dict(doc.meta)
                new_meta["chunk_index"] = idx
                new_meta["source_doc_id"] = doc.id
                result.append(Document(content=chunk, meta=new_meta))
        return {"documents": result}

    def _split_text(self, text: str) -> List[str]:
        """å¥ç‚¹ãƒ»æ”¹è¡Œã§æ–‡åˆ†å‰²ã—ã€chunk_size æ–‡å­—ã”ã¨ã«ã¾ã¨ã‚ã‚‹"""
        # æ—¥æœ¬èªã®æ–‡æœ«è¨˜å·ã§åˆ†å‰²ï¼ˆåŒºåˆ‡ã‚Šæ–‡å­—ã¯å‰ã®æ–‡ã«å«ã‚ã‚‹ï¼‰
        sentences = re.split(r"(?<=[ã€‚\uFF01\uFF1F\n])", text)
        sentences = [s for s in sentences if s.strip()]
        if not sentences:
            return [text] if text.strip() else []

        chunks = []
        current = ""
        for sentence in sentences:
            if len(current) + len(sentence) > self.chunk_size and current:
                chunks.append(current.strip())
                # ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—: ç¾ãƒãƒ£ãƒ³ã‚¯æœ«å°¾ã‚’æ¬¡ãƒãƒ£ãƒ³ã‚¯ã®å…ˆé ­ã«å¼•ãç¶™ã
                if len(current) > self.chunk_overlap:
                    current = current[-self.chunk_overlap :] + sentence
                else:
                    current = current + sentence
            else:
                current += sentence

        if current.strip():
            chunks.append(current.strip())

        return chunks


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  PDF èª­ã¿è¾¼ã¿
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def load_pdf_with_pymupdf(file_path: str) -> List[Document]:
    """PyMuPDF ã§ PDF ã‚’èª­ã¿è¾¼ã¿ã€ãƒšãƒ¼ã‚¸ã”ã¨ã« Haystack Document ã‚’ç”Ÿæˆ"""
    pdf = fitz.open(file_path)
    documents = []
    for page_num, page in enumerate(pdf):
        text = page.get_text()
        if text.strip():
            documents.append(
                Document(
                    content=text,
                    meta={
                        "file_name": Path(file_path).name,
                        "file_path": file_path,
                        "page_number": page_num + 1,
                    },
                )
            )
    pdf.close()
    return documents


def load_all_documents() -> List[Document]:
    """data/ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªé…ä¸‹ã®å…¨ PDF ã‚’èª­ã¿è¾¼ã‚€"""
    if not DATA_DIR.exists() or not any(DATA_DIR.iterdir()):
        raise FileNotFoundError(
            f"âš ï¸  {DATA_DIR} ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“ã€‚\n"
            "   PDF ã‚’é…ç½®ã—ã¦ã‹ã‚‰å†å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚"
        )

    all_docs = []
    for fp in sorted(DATA_DIR.iterdir()):
        if fp.suffix.lower() == ".pdf":
            try:
                docs = load_pdf_with_pymupdf(str(fp))
                all_docs.extend(docs)
                print(f"  âœ… {fp.name}: {len(docs)} ãƒšãƒ¼ã‚¸")
            except Exception as e:
                print(f"  âŒ {fp.name}: èª­ã¿è¾¼ã¿å¤±æ•— ({e})")

    print(f"\n  åˆè¨ˆ {len(all_docs)} ãƒšãƒ¼ã‚¸åˆ†ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")
    return all_docs


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  ã‚¤ãƒ³ã‚¸ã‚§ã‚¹ã‚·ãƒ§ãƒ³ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def build_indexing_pipeline(document_store: ChromaDocumentStore) -> Pipeline:
    """
    ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå–ã‚Šè¾¼ã¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ã™ã‚‹ã€‚

    ãƒ•ãƒ­ãƒ¼:
      å…¥åŠ›ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ (PyMuPDF ã§èª­ã¿è¾¼ã‚“ã ç”Ÿãƒšãƒ¼ã‚¸)
        â†’ log_input      : ãƒ­ã‚°å‡ºåŠ›ï¼ˆç”Ÿãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®çŠ¶æ…‹ã‚’ç¢ºèªï¼‰
        â†’ cleaner        : ç©ºè¡Œãƒ»ä½™åˆ†ãªç©ºç™½ã‚’é™¤å»
        â†’ log_cleaned    : ãƒ­ã‚°å‡ºåŠ›ï¼ˆã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å¾Œã®çŠ¶æ…‹ã‚’ç¢ºèªï¼‰
        â†’ splitter       : æ—¥æœ¬èªå¯¾å¿œãƒãƒ£ãƒ³ã‚¯åˆ†å‰² (1000æ–‡å­—, 300æ–‡å­—ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—)
        â†’ log_split      : ãƒ­ã‚°å‡ºåŠ›ï¼ˆåˆ†å‰²å¾Œã®ãƒãƒ£ãƒ³ã‚¯æ•°ãƒ»å†…å®¹ã‚’ç¢ºèªï¼‰
        â†’ doc_embedder   : OpenAI Embedding ã§ãƒ™ã‚¯ãƒˆãƒ«åŒ–
        â†’ log_embedded   : ãƒ­ã‚°å‡ºåŠ›ï¼ˆã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°å¾Œã®çŠ¶æ…‹ã‚’ç¢ºèªï¼‰
        â†’ writer         : InMemoryDocumentStore ã¸æ›¸ãè¾¼ã¿
    """
    pipe = Pipeline()

    # â”€â”€ ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆç™»éŒ² â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    pipe.add_component(
        "log_input",
        DocumentLogger("1. å…¥åŠ›ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼ˆç”ŸPDFï¼‰"),
    )
    pipe.add_component(
        "cleaner",
        DocumentCleaner(
            remove_empty_lines=True,
            remove_extra_whitespaces=True,
        ),
    )
    pipe.add_component(
        "log_cleaned",
        DocumentLogger("2. ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å¾Œ"),
    )
    pipe.add_component(
        "splitter",
        JapaneseDocumentSplitter(
            chunk_size=CHUNK_SIZE,
            chunk_overlap=CHUNK_OVERLAP,
        ),
    )
    pipe.add_component(
        "log_split",
        DocumentLogger("3. ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²å¾Œ"),
    )
    pipe.add_component(
        "doc_embedder",
        OpenAIDocumentEmbedder(model=EMBEDDING_MODEL),
    )
    pipe.add_component(
        "log_embedded",
        DocumentLogger("4. ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°å¾Œ"),
    )
    pipe.add_component(
        "writer",
        DocumentWriter(document_store=document_store),
    )

    # â”€â”€ ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆæ¥ç¶š â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # å„ã‚¹ãƒ†ãƒƒãƒ—ã‚’ç›´åˆ—ã«æ¥ç¶šã€‚ãƒ­ã‚°ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã¯ãƒ‘ã‚¹ã‚¹ãƒ«ãƒ¼ã¨ã—ã¦é–“ã«æŒŸã‚€ã€‚
    pipe.connect("log_input.documents", "cleaner.documents")
    pipe.connect("cleaner.documents", "log_cleaned.documents")
    pipe.connect("log_cleaned.documents", "splitter.documents")
    pipe.connect("splitter.documents", "log_split.documents")
    pipe.connect("log_split.documents", "doc_embedder.documents")
    pipe.connect("doc_embedder.documents", "log_embedded.documents")
    pipe.connect("log_embedded.documents", "writer.documents")

    return pipe


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  ã‚¯ã‚¨ãƒªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def build_query_pipeline(document_store: ChromaDocumentStore) -> Pipeline:
    """
    è³ªå•å¿œç­”ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ã™ã‚‹ã€‚

    ãƒ•ãƒ­ãƒ¼:
      ãƒ¦ãƒ¼ã‚¶ãƒ¼è³ªå• (query)
        â†’ log_query        : ãƒ­ã‚°å‡ºåŠ›ï¼ˆå—ä¿¡ã‚¯ã‚¨ãƒªã‚’ç¢ºèªï¼‰
        â”œâ†’ text_embedder   : OpenAI Embedding ã§ã‚¯ã‚¨ãƒªã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–
        â”‚   â†’ retriever    : ãƒ™ã‚¯ãƒˆãƒ«é¡ä¼¼åº¦æ¤œç´¢ (top_k=10)
        â”‚       â†’ log_retrieved : ãƒ­ã‚°å‡ºåŠ›ï¼ˆæ¤œç´¢çµæœãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ç¢ºèªï¼‰
        â”‚           â†’ prompt_builder : Jinja ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã§ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆçµ„ã¿ç«‹ã¦
        â””â†’ prompt_builder    â† query ã‚‚ç›´æ¥æ¸¡ã™
              â†’ llm            : OpenAI gpt-4o ã§å›ç­”ç”Ÿæˆ (temperature=0.1)
                  â†’ log_response : ãƒ­ã‚°å‡ºåŠ›ï¼ˆç”Ÿæˆçµæœã‚’ç¢ºèªï¼‰
    """
    pipe = Pipeline()

    # â”€â”€ ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆç™»éŒ² â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    pipe.add_component(
        "log_query",
        QueryLogger("1. ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¯ã‚¨ãƒªå—ä¿¡"),
    )
    pipe.add_component(
        "text_embedder",
        OpenAITextEmbedder(model=EMBEDDING_MODEL),
    )
    pipe.add_component(
        "retriever",
        ChromaEmbeddingRetriever(
            document_store=document_store,
            top_k=TOP_K,
        ),
    )
    pipe.add_component(
        "log_retrieved",
        DocumentLogger("2. ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢çµæœ"),
    )
    pipe.add_component(
        "prompt_builder",
        PromptBuilder(template=RAG_PROMPT_TEMPLATE),
    )
    pipe.add_component(
        "llm",
        OpenAIGenerator(
            model="gpt-4o",
            generation_kwargs={"temperature": 0.1},
        ),
    )
    pipe.add_component(
        "log_response",
        GenerationLogger("3. LLM ç”Ÿæˆçµæœ"),
    )

    # â”€â”€ ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆæ¥ç¶š â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # query ã¯ log_query ã‹ã‚‰ text_embedder ã¨ prompt_builder ã®ä¸¡æ–¹ã¸åˆ†å²
    pipe.connect("log_query.query", "text_embedder.text")
    pipe.connect("log_query.query", "prompt_builder.query")

    # ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚° â†’ æ¤œç´¢ â†’ ãƒ­ã‚° â†’ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ“ãƒ«ãƒ€ãƒ¼ â†’ LLM â†’ ãƒ­ã‚°
    pipe.connect("text_embedder.embedding", "retriever.query_embedding")
    pipe.connect("retriever.documents", "log_retrieved.documents")
    pipe.connect("log_retrieved.documents", "prompt_builder.documents")
    pipe.connect("prompt_builder.prompt", "llm.prompt")
    pipe.connect("llm.replies", "log_response.replies")

    return pipe


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  ãƒãƒ£ãƒƒãƒˆãƒ«ãƒ¼ãƒ—
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def chat_loop(query_pipeline: Pipeline):
    """CLI ãƒãƒ£ãƒƒãƒˆãƒ«ãƒ¼ãƒ—"""
    print("\n" + "=" * 50)
    print("RAG ãƒãƒ£ãƒƒãƒˆã‚·ã‚¹ãƒ†ãƒ  (Haystack 2.x + ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢)")
    print("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆçµ‚äº†ã™ã‚‹ã«ã¯ 'exit' ã¨å…¥åŠ›ï¼‰")
    print("=" * 50 + "\n")

    while True:
        try:
            user_input = input("ã‚ãªãŸ: ").strip()

            if not user_input:
                continue

            if user_input.lower() == "exit":
                print("ãƒãƒ£ãƒƒãƒˆã‚’çµ‚äº†ã—ã¾ã™ã€‚")
                break

            # â”€â”€â”€ ã‚¯ã‚¨ãƒªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            result = query_pipeline.run(
                {
                    "log_query": {"query": user_input},
                    "prompt_builder": {"system_prompt": SYSTEM_PROMPT},
                },
                include_outputs_from={"log_retrieved"},
            )

            # â”€â”€â”€ å›ç­”è¡¨ç¤º â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            replies = result.get("log_response", {}).get("replies", [])
            if replies:
                print(f"\nã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ: {replies[0]}\n")

            # â”€â”€â”€ å‚ç…§ã‚½ãƒ¼ã‚¹è¡¨ç¤º â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            retrieved = result.get("log_retrieved", {}).get("documents", [])
            if retrieved:
                print("--- å‚ç…§ã‚½ãƒ¼ã‚¹ ---")
                for i, doc in enumerate(retrieved, 1):
                    fname = doc.meta.get("file_name", "ä¸æ˜")
                    page = doc.meta.get("page_number", "N/A")
                    score = doc.score
                    if isinstance(score, float):
                        print(f"  [{i}] {fname} (p.{page}, é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢: {score:.4f})")
                    else:
                        print(f"  [{i}] {fname} (p.{page})")
                print("------------------\n")

        except KeyboardInterrupt:
            print("\n\nãƒãƒ£ãƒƒãƒˆã‚’çµ‚äº†ã—ã¾ã™ã€‚")
            break
        except Exception as e:
            print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}\n")
            pipeline_logger.exception("ã‚¯ã‚¨ãƒªå®Ÿè¡Œæ™‚ã‚¨ãƒ©ãƒ¼")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  ãƒ¡ã‚¤ãƒ³
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def main():
    parser = argparse.ArgumentParser(description="RAG ãƒãƒ£ãƒƒãƒˆ (Haystack 2.x + ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢)")
    parser.add_argument(
        "--rebuild",
        action="store_true",
        help="ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å†æ§‹ç¯‰ã™ã‚‹ï¼ˆæ—¢å­˜ã®ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚’å‰Šé™¤ï¼‰",
    )
    args = parser.parse_args()

    # â”€â”€ API ã‚­ãƒ¼ç¢ºèª â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if not os.getenv("OPENAI_API_KEY"):
        print("âŒ ã‚¨ãƒ©ãƒ¼: OPENAI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        print("   .env ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã€APIã‚­ãƒ¼ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
        return

    # â”€â”€ --rebuild ã‚ªãƒ—ã‚·ãƒ§ãƒ³ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if args.rebuild and STORAGE_DIR.exists():
        print("ğŸ—‘ï¸  æ—¢å­˜ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å‰Šé™¤ã—ã¦ã„ã¾ã™â€¦")
        shutil.rmtree(STORAGE_DIR)
        print("âœ… å‰Šé™¤å®Œäº†ã€‚å†æ§‹ç¯‰ã—ã¾ã™ã€‚")

    try:
        # â”€â”€ 1. ChromaDocumentStore åˆæœŸåŒ– â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        print(f"ğŸ“¦ ChromaDocumentStore ã‚’åˆæœŸåŒ–ä¸­â€¦ (æ°¸ç¶šåŒ–å…ˆ: {STORAGE_DIR})")
        document_store = ChromaDocumentStore(
            collection_name=COLLECTION_NAME,
            persist_path=str(STORAGE_DIR),
        )

        # â”€â”€ 2. æ—¢å­˜ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ç¢ºèª â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        existing_count = document_store.count_documents()
        if existing_count > 0:
            print(f"ğŸ“‚ æ—¢å­˜ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚({existing_count} ä»¶ã®ãƒãƒ£ãƒ³ã‚¯)")
        else:
            # â”€â”€ 3. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆèª­ã¿è¾¼ã¿ãƒ»ã‚¤ãƒ³ã‚¸ã‚§ã‚¹ã‚·ãƒ§ãƒ³ â”€â”€â”€â”€â”€â”€
            print(f"ğŸ“„ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã¿ä¸­â€¦ (ã‚½ãƒ¼ã‚¹: {DATA_DIR})")
            documents = load_all_documents()

            print("\nğŸ”§ ã‚¤ãƒ³ã‚¸ã‚§ã‚¹ã‚·ãƒ§ãƒ³ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ä¸­â€¦")
            indexing_pipeline = build_indexing_pipeline(document_store)

            print("\nğŸ“Š ã‚¤ãƒ³ã‚¸ã‚§ã‚¹ã‚·ãƒ§ãƒ³ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹é€ :")
            print(indexing_pipeline)

            print("\nğŸš€ ã‚¤ãƒ³ã‚¸ã‚§ã‚¹ã‚·ãƒ§ãƒ³ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè¡Œä¸­â€¦")
            indexing_result = indexing_pipeline.run(
                {"log_input": {"documents": documents}}
            )
            written = indexing_result.get("writer", {}).get("documents_written", 0)
            print(f"\nâœ… DocumentStore ã« {written} ä»¶ã®ãƒãƒ£ãƒ³ã‚¯ã‚’æ ¼ç´ã—ã¾ã—ãŸã€‚")
            print(f"   (DocumentStore å†…ã®ç·ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {document_store.count_documents()})")

        # â”€â”€ 4. ã‚¯ã‚¨ãƒªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        print("\nğŸ”§ ã‚¯ã‚¨ãƒªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ä¸­â€¦")
        query_pipeline = build_query_pipeline(document_store)

        print("\nğŸ“Š ã‚¯ã‚¨ãƒªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹é€ :")
        print(query_pipeline)

        # â”€â”€ 5. ãƒãƒ£ãƒƒãƒˆãƒ«ãƒ¼ãƒ— â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        chat_loop(query_pipeline)

    except FileNotFoundError as e:
        print(f"âŒ {e}")
    except Exception as e:
        print(f"âŒ äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        raise


if __name__ == "__main__":
    main()
