"""
Haystack 2.x RAG è©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

LLM ã‚’ä½¿ã£ã¦ Faithfulnessï¼ˆå¿ å®Ÿæ€§ï¼‰ã¨ Answer Relevancyï¼ˆå›ç­”é–¢é€£æ€§ï¼‰ã‚’è©•ä¾¡ã™ã‚‹ã€‚
mark-1/evaluate.py ã¨åŒç­‰ã®æ©Ÿèƒ½ã‚’ Haystack 2.x ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä¸Šã§å†å®Ÿè£…ã€‚
"""

import argparse
import json
import os
import time
from pathlib import Path
from typing import Optional

from dotenv import load_dotenv
from openai import OpenAI

# ãƒ¡ã‚¤ãƒ³ã® RAG ã‚·ã‚¹ãƒ†ãƒ ã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from main import (
    DATA_DIR,
    JAPANESE_BM25_REGEX,
    SYSTEM_PROMPT,
    BM25_TOP_K,
    CHUNK_SIZE,
    CHUNK_OVERLAP,
    build_indexing_pipeline,
    build_query_pipeline,
    load_all_documents,
    pipeline_logger,
)
from haystack.document_stores.in_memory import InMemoryDocumentStore

# â”€â”€â”€ ç’°å¢ƒå¤‰æ•° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
load_dotenv()
if not os.getenv("OPENAI_API_KEY"):
    _fallback_env = Path(__file__).resolve().parent.parent / "rag_mark-1" / ".env"
    if _fallback_env.exists():
        load_dotenv(_fallback_env)

# â”€â”€â”€ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TEST_DATA_FILE = Path("./test_cases.json")
# ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: rag_mark-1 ã®ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹
if not TEST_DATA_FILE.exists():
    _fallback_tests = Path(__file__).resolve().parent.parent / "rag_mark-1" / "test_cases.json"
    if _fallback_tests.exists():
        TEST_DATA_FILE = _fallback_tests


def load_test_cases() -> list[dict]:
    """ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã®èª­ã¿è¾¼ã¿"""
    if not TEST_DATA_FILE.exists():
        sample_cases = [
            {
                "input": "ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯ä½•ã«ã¤ã„ã¦æ›¸ã‹ã‚Œã¦ã„ã¾ã™ã‹ï¼Ÿ",
                "expected_output": None,
            },
        ]
        with open(TEST_DATA_FILE, "w", encoding="utf-8") as f:
            json.dump(sample_cases, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“ ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’ {TEST_DATA_FILE} ã«ä½œæˆã—ã¾ã—ãŸã€‚")
        return sample_cases

    with open(TEST_DATA_FILE, "r", encoding="utf-8") as f:
        return json.load(f)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  RAG ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def initialize_rag_system():
    """
    Haystack ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ã—ã€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã‚“ã§ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–ã™ã‚‹ã€‚
    Returns: (query_pipeline, document_store)
    """
    import logging
    # è©•ä¾¡æ™‚ã¯ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒ­ã‚°ã‚’æŠ‘åˆ¶
    pipeline_logger.setLevel(logging.WARNING)

    print("ğŸ“¦ InMemoryDocumentStore ã‚’åˆæœŸåŒ–ä¸­â€¦")
    document_store = InMemoryDocumentStore(
        bm25_tokenization_regex=JAPANESE_BM25_REGEX,
    )

    print(f"ğŸ“„ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã¿ä¸­â€¦ (ã‚½ãƒ¼ã‚¹: {DATA_DIR})")
    documents = load_all_documents()

    print("ğŸ”§ ã‚¤ãƒ³ã‚¸ã‚§ã‚¹ã‚·ãƒ§ãƒ³ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè¡Œä¸­â€¦")
    indexing_pipeline = build_indexing_pipeline(document_store)
    indexing_result = indexing_pipeline.run({"log_input": {"documents": documents}})
    written = indexing_result.get("writer", {}).get("documents_written", 0)
    print(f"âœ… {written} ä»¶ã®ãƒãƒ£ãƒ³ã‚¯ã‚’æ ¼ç´ã—ã¾ã—ãŸã€‚")

    print("ğŸ”§ ã‚¯ã‚¨ãƒªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ä¸­â€¦")
    query_pipeline = build_query_pipeline(document_store)
    print("âœ… åˆæœŸåŒ–å®Œäº†\n")

    return query_pipeline


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  RAG ã‚¯ã‚¨ãƒªå®Ÿè¡Œ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def run_rag_query(query_pipeline, question: str) -> tuple[str, list[str]]:
    """
    Haystack ã‚¯ã‚¨ãƒªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè¡Œã—ã€å›ç­”ã¨å‚ç…§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿”ã™ã€‚
    """
    result = query_pipeline.run(
        {
            "log_query": {"query": question},
            "prompt_builder": {"system_prompt": SYSTEM_PROMPT},
        },
        include_outputs_from={"log_retrieved"},
    )

    # å›ç­”ã®å–å¾—
    replies = result.get("log_response", {}).get("replies", [])
    answer = replies[0] if replies else ""

    # å‚ç…§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®å–å¾—
    contexts = []
    retrieved_docs = result.get("log_retrieved", {}).get("documents", [])
    for doc in retrieved_docs:
        contexts.append(doc.content)

    return answer, contexts


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  LLM è©•ä¾¡
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def evaluate_with_llm(
    question: str,
    answer: str,
    contexts: list[str],
    expected: Optional[str],
) -> dict:
    """
    OpenAI API ã§ Faithfulness / Answer Relevancy ã‚’è©•ä¾¡ã™ã‚‹ã€‚
    """
    client = OpenAI()
    context_text = "\n---\n".join(contexts)

    # â”€â”€ Faithfulness è©•ä¾¡ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ â”€â”€
    faithfulness_prompt = f"""ä»¥ä¸‹ã®å›ç­”ãŒã€æä¾›ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«å¿ å®Ÿã‹ã©ã†ã‹ã‚’è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚
å›ç­”ãŒã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«å«ã¾ã‚Œã‚‹æƒ…å ±ã®ã¿ã«åŸºã¥ã„ã¦ã„ã‚‹ã‹ã€å¹»è¦šï¼ˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«ãªã„æƒ…å ±ã®è¿½åŠ ï¼‰ãŒãªã„ã‹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚

ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ:
{context_text}

è³ªå•: {question}
å›ç­”: {answer}

è©•ä¾¡çµæœã‚’JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„:
{{"score": 0.0ã‹ã‚‰1.0ã®æ•°å€¤, "reason": "è©•ä¾¡ç†ç”±"}}

ã‚¹ã‚³ã‚¢ã®åŸºæº–:
- 1.0: å›ç­”ã¯å®Œå…¨ã«ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«åŸºã¥ã„ã¦ã„ã‚‹
- 0.7-0.9: å›ç­”ã¯ã»ã¼ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«åŸºã¥ã„ã¦ã„ã‚‹ãŒã€è»½å¾®ãªæ¨è«–ã‚’å«ã‚€
- 0.4-0.6: å›ç­”ã®ä¸€éƒ¨ãŒã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«åŸºã¥ã„ã¦ã„ãªã„
- 0.0-0.3: å›ç­”ã®å¤§éƒ¨åˆ†ãŒã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«åŸºã¥ã„ã¦ã„ãªã„ï¼ˆå¹»è¦šãŒå¤šã„ï¼‰
"""

    # â”€â”€ Answer Relevancy è©•ä¾¡ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ â”€â”€
    relevancy_prompt = f"""ä»¥ä¸‹ã®å›ç­”ãŒã€è³ªå•ã«å¯¾ã—ã¦é©åˆ‡ã‹ã©ã†ã‹ã‚’è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚
å›ç­”ãŒè³ªå•ã«ç›´æ¥ç­”ãˆã¦ã„ã‚‹ã‹ã€é–¢é€£æ€§ãŒã‚ã‚‹ã‹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚

è³ªå•: {question}
å›ç­”: {answer}
{f"æœŸå¾…ã•ã‚Œã‚‹å›ç­”: {expected}" if expected else ""}

è©•ä¾¡çµæœã‚’JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„:
{{"score": 0.0ã‹ã‚‰1.0ã®æ•°å€¤, "reason": "è©•ä¾¡ç†ç”±"}}

ã‚¹ã‚³ã‚¢ã®åŸºæº–:
- 1.0: å›ç­”ã¯è³ªå•ã«å®Œå…¨ã«ç­”ãˆã¦ã„ã‚‹
- 0.7-0.9: å›ç­”ã¯è³ªå•ã«ã»ã¼ç­”ãˆã¦ã„ã‚‹ãŒã€ä¸€éƒ¨ä¸è¶³ãŒã‚ã‚‹
- 0.4-0.6: å›ç­”ã¯éƒ¨åˆ†çš„ã«ã—ã‹è³ªå•ã«ç­”ãˆã¦ã„ãªã„
- 0.0-0.3: å›ç­”ã¯è³ªå•ã«ç­”ãˆã¦ã„ãªã„
"""

    results = {
        "faithfulness": {"score": None, "reason": None, "error": None},
        "relevancy": {"score": None, "reason": None, "error": None},
    }

    def _call_llm(prompt: str) -> dict:
        """OpenAI API ã‚’å‘¼ã³å‡ºã—ã€JSON ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è§£æ"""
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.1,
        )
        text = response.choices[0].message.content.strip()
        if "{" in text and "}" in text:
            json_str = text[text.find("{"):text.rfind("}") + 1]
            return json.loads(json_str)
        return {}

    # Faithfulness è©•ä¾¡
    try:
        data = _call_llm(faithfulness_prompt)
        results["faithfulness"]["score"] = float(data.get("score", 0))
        results["faithfulness"]["reason"] = data.get("reason", "")
    except Exception as e:
        results["faithfulness"]["error"] = str(e)

    time.sleep(1)

    # Answer Relevancy è©•ä¾¡
    try:
        data = _call_llm(relevancy_prompt)
        results["relevancy"]["score"] = float(data.get("score", 0))
        results["relevancy"]["reason"] = data.get("reason", "")
    except Exception as e:
        results["relevancy"]["error"] = str(e)

    return results


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  ä¸€æ‹¬è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def run_simple_evaluation(verbose: bool = True):
    """ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’ä¸€æ‹¬ã§è©•ä¾¡"""
    print("=" * 60)
    print("RAG è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ï¼ˆHaystack 2.x + BM25 / LLM ç›´æ¥è©•ä¾¡ï¼‰")
    print("=" * 60)

    # RAG åˆæœŸåŒ–
    query_pipeline = initialize_rag_system()

    # ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹èª­ã¿è¾¼ã¿
    print(f"ğŸ“‹ ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’èª­ã¿è¾¼ã¿ä¸­â€¦ ({TEST_DATA_FILE})")
    test_data = load_test_cases()
    print(f"   {len(test_data)} ä»¶ã®ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")

    results_summary = []
    total_faithfulness = 0
    total_relevancy = 0
    valid_count = 0

    for i, data in enumerate(test_data, 1):
        question = data["input"]
        expected = data.get("expected_output")

        print(f"\n{'='*60}")
        print(f"[{i}/{len(test_data)}] è³ªå•: {question}")
        print("=" * 60)

        # RAG ã‚¯ã‚¨ãƒªå®Ÿè¡Œ
        answer, contexts = run_rag_query(query_pipeline, question)
        print(f"\nğŸ“ å›ç­”:\n{answer}\n")

        # LLM è©•ä¾¡
        print("ğŸ“Š è©•ä¾¡ä¸­â€¦")
        eval_results = evaluate_with_llm(question, answer, contexts, expected)

        # Faithfulness çµæœ
        f_score = eval_results["faithfulness"]["score"]
        f_reason = eval_results["faithfulness"]["reason"]
        f_error = eval_results["faithfulness"]["error"]

        print(f"\nã€Faithfulnessï¼ˆå¿ å®Ÿæ€§ï¼‰ã€‘")
        if f_error:
            print(f"   âš ï¸ ã‚¨ãƒ©ãƒ¼: {f_error}")
        elif f_score is not None:
            status = "âœ… PASS" if f_score >= 0.7 else "âŒ FAIL"
            print(f"   ã‚¹ã‚³ã‚¢: {f_score:.2f} {status}")
            print(f"   ç†ç”±: {f_reason}")
            total_faithfulness += f_score
        else:
            print("   âš ï¸ ã‚¹ã‚³ã‚¢å–å¾—å¤±æ•—")

        # Answer Relevancy çµæœ
        r_score = eval_results["relevancy"]["score"]
        r_reason = eval_results["relevancy"]["reason"]
        r_error = eval_results["relevancy"]["error"]

        print(f"\nã€Answer Relevancyï¼ˆå›ç­”é–¢é€£æ€§ï¼‰ã€‘")
        if r_error:
            print(f"   âš ï¸ ã‚¨ãƒ©ãƒ¼: {r_error}")
        elif r_score is not None:
            status = "âœ… PASS" if r_score >= 0.7 else "âŒ FAIL"
            print(f"   ã‚¹ã‚³ã‚¢: {r_score:.2f} {status}")
            print(f"   ç†ç”±: {r_reason}")
            total_relevancy += r_score
        else:
            print("   âš ï¸ ã‚¹ã‚³ã‚¢å–å¾—å¤±æ•—")

        if f_score is not None and r_score is not None:
            valid_count += 1

        results_summary.append({
            "question": question,
            "answer": answer,
            "expected": expected,
            "faithfulness": eval_results["faithfulness"],
            "relevancy": eval_results["relevancy"],
        })

        time.sleep(1)

    # â”€â”€ ã‚µãƒãƒªãƒ¼ â”€â”€
    print("\n" + "=" * 60)
    print("ğŸ“ˆ è©•ä¾¡çµæœã‚µãƒãƒªãƒ¼")
    print("=" * 60)

    if valid_count > 0:
        avg_faithfulness = total_faithfulness / valid_count
        avg_relevancy = total_relevancy / valid_count
        print(f"\nè©•ä¾¡å®Œäº†: {valid_count}/{len(test_data)} ä»¶")
        print(f"\nå¹³å‡ã‚¹ã‚³ã‚¢:")
        print(f"  Faithfulness:     {avg_faithfulness:.2f} {'âœ…' if avg_faithfulness >= 0.7 else 'âŒ'}")
        print(f"  Answer Relevancy: {avg_relevancy:.2f} {'âœ…' if avg_relevancy >= 0.7 else 'âŒ'}")
    else:
        print("\nâš ï¸ æœ‰åŠ¹ãªè©•ä¾¡çµæœãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

    # çµæœä¿å­˜
    output_file = Path("./evaluation_results.json")
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(results_summary, f, ensure_ascii=False, indent=2)
    print(f"\nğŸ“ è©³ç´°çµæœã‚’ {output_file} ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")

    return results_summary


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  å¯¾è©±è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def interactive_evaluation():
    """å¯¾è©±å½¢å¼ã§å˜ä¸€ã®è³ªå•ã‚’è©•ä¾¡"""
    print("=" * 60)
    print("RAG å¯¾è©±è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ (Haystack 2.x + BM25)")
    print("=" * 60)

    query_pipeline = initialize_rag_system()

    print("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆçµ‚äº†ã™ã‚‹ã«ã¯ 'exit'ï¼‰")
    print("-" * 60)

    while True:
        try:
            question = input("\nè³ªå•: ").strip()

            if not question:
                continue

            if question.lower() == "exit":
                print("è©•ä¾¡ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
                break

            # RAG ã‚¯ã‚¨ãƒªå®Ÿè¡Œ
            print("\nğŸ” å›ç­”ã‚’ç”Ÿæˆä¸­â€¦")
            answer, contexts = run_rag_query(query_pipeline, question)

            print(f"\nğŸ“ å›ç­”:\n{answer}")
            print(f"\nğŸ“š å‚ç…§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ•°: {len(contexts)}")

            # è©•ä¾¡
            print("\nğŸ“Š è©•ä¾¡ä¸­â€¦")
            eval_results = evaluate_with_llm(question, answer, contexts, None)

            # çµæœè¡¨ç¤º
            f_score = eval_results["faithfulness"]["score"]
            r_score = eval_results["relevancy"]["score"]

            print(f"\nã€Faithfulnessã€‘ ã‚¹ã‚³ã‚¢: {f_score:.2f}" if f_score else "\nã€Faithfulnessã€‘ ã‚¹ã‚³ã‚¢: N/A")
            if eval_results["faithfulness"]["reason"]:
                print(f"   ç†ç”±: {eval_results['faithfulness']['reason']}")

            print(f"\nã€Answer Relevancyã€‘ ã‚¹ã‚³ã‚¢: {r_score:.2f}" if r_score else "\nã€Answer Relevancyã€‘ ã‚¹ã‚³ã‚¢: N/A")
            if eval_results["relevancy"]["reason"]:
                print(f"   ç†ç”±: {eval_results['relevancy']['reason']}")

            print("\n" + "-" * 60)

        except KeyboardInterrupt:
            print("\n\nè©•ä¾¡ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
            break
        except Exception as e:
            print(f"\nâŒ ã‚¨ãƒ©ãƒ¼: {e}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  ãƒ¡ã‚¤ãƒ³
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="RAG è©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆ (Haystack 2.x)")
    parser.add_argument(
        "--mode",
        choices=["simple", "interactive"],
        default="simple",
        help="è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰: simpleï¼ˆä¸€æ‹¬è©•ä¾¡ï¼‰ã€interactiveï¼ˆå¯¾è©±è©•ä¾¡ï¼‰",
    )
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="è©³ç´°å‡ºåŠ›ã‚’æœ‰åŠ¹ã«ã™ã‚‹",
    )

    args = parser.parse_args()

    if not os.getenv("OPENAI_API_KEY"):
        print("âŒ OPENAI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    elif args.mode == "simple":
        run_simple_evaluation(verbose=args.verbose)
    else:
        interactive_evaluation()
